{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# EDA and Data Transformations\n",
    "\n",
    "**Author:** Alexis Alduncin (Data Scientist)\n",
    "**Team:** MLOps 62\n",
    "\n",
    "This notebook performs comprehensive Exploratory Data Analysis and applies the transformation pipeline using our custom modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": "# Setup and imports\nimport sys\nsys.path.append('..')\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Import our custom modules\nfrom src import config\nfrom src.features import AbsenteeismFeatureEngine\nfrom src.data_utils import load_data\nfrom src.plots import (\n    plot_target_distribution,\n    plot_correlation_matrix,\n    create_eda_summary_dashboard,\n    plot_categorical_analysis,\n    plot_numerical_relationship\n)\n\nprint(\"✅ Modules imported successfully\")\nprint(f\"MLflow Experiment: {config.MLFLOW_EXPERIMENT_NAME}\")\nprint(f\"Data Source: {config.RAW_DATA_PATH}\")"
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 1. Data Loading with DVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": "# Load data using team's robust DVC approach\ndf = load_data(config.RAW_DATA_PATH)\n\nprint(f\"Dataset loaded: {df.shape}\")\nprint(f\"Features: {df.shape[1]}\")\nprint(f\"Samples: {df.shape[0]}\")\n\n# Display first few rows\ndf.head()"
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 2. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic info\n",
    "print(\"=\"*60)\n",
    "print(\"DATA QUALITY REPORT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nShape: {df.shape}\")\n",
    "print(f\"\\nMissing Values:\")\n",
    "missing = df.isnull().sum()\n",
    "if missing.sum() == 0:\n",
    "    print(\"  ✅ No missing values\")\n",
    "else:\n",
    "    print(missing[missing > 0])\n",
    "\n",
    "print(f\"\\nDuplicates: {df.duplicated().sum()}\")\n",
    "\n",
    "print(f\"\\nData Types:\")\n",
    "print(df.dtypes.value_counts())\n",
    "\n",
    "# Summary statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## 3. Target Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target distribution using our custom plot\n",
    "fig, axes = plot_target_distribution(df, config.TARGET_COLUMN)\n",
    "plt.show()\n",
    "\n",
    "# Target statistics\n",
    "print(\"\\nTarget Variable Statistics:\")\n",
    "print(f\"Mean: {df[config.TARGET_COLUMN].mean():.2f} hours\")\n",
    "print(f\"Median: {df[config.TARGET_COLUMN].median():.2f} hours\")\n",
    "print(f\"Std Dev: {df[config.TARGET_COLUMN].std():.2f} hours\")\n",
    "print(f\"Min: {df[config.TARGET_COLUMN].min():.0f} hours\")\n",
    "print(f\"Max: {df[config.TARGET_COLUMN].max():.0f} hours\")\n",
    "\n",
    "# Check for outliers\n",
    "Q1 = df[config.TARGET_COLUMN].quantile(0.25)\n",
    "Q3 = df[config.TARGET_COLUMN].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers = df[(df[config.TARGET_COLUMN] < Q1 - 1.5*IQR) | \n",
    "              (df[config.TARGET_COLUMN] > Q3 + 1.5*IQR)]\n",
    "print(f\"\\nOutliers (IQR method): {len(outliers)} ({len(outliers)/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 4. Comprehensive EDA Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive EDA dashboard\n",
    "fig = create_eda_summary_dashboard(df)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ EDA Dashboard created with 7 visualizations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## 5. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "fig = plot_correlation_matrix(df, method='pearson')\n",
    "plt.show()\n",
    "\n",
    "# Top correlations with target\n",
    "numeric_df = df.select_dtypes(include=[np.number])\n",
    "target_corr = numeric_df.corr()[config.TARGET_COLUMN].sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Correlations with Target:\")\n",
    "print(target_corr.head(11)[1:])  # Exclude self-correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## 6. Categorical Features Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze key categorical features\n",
    "categorical_features = ['Day of the week', 'Seasons', 'Education']\n",
    "\n",
    "for feature in categorical_features:\n",
    "    if feature in df.columns:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Analyzing: {feature}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        fig = plot_categorical_analysis(df, feature)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## 7. Numerical Features Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze key numerical features\n",
    "numerical_features = ['Age', 'Distance from Residence to Work', 'Body mass index']\n",
    "\n",
    "for feature in numerical_features:\n",
    "    if feature in df.columns:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Analyzing: {feature}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        fig = plot_numerical_relationship(df, feature)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## 8. Data Transformation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize feature engine\n",
    "engine = AbsenteeismFeatureEngine()\n",
    "\n",
    "print(\"Step 1: Data Cleaning\")\n",
    "df_clean = engine.clean_data(df)\n",
    "print(f\"  Original: {len(df)} rows\")\n",
    "print(f\"  Cleaned: {len(df_clean)} rows\")\n",
    "print(f\"  Removed: {len(df) - len(df_clean)} rows ({(len(df)-len(df_clean))/len(df)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nStep 2: Feature Engineering\")\n",
    "df_features = engine.engineer_features(df_clean)\n",
    "print(f\"  Original features: {len(df.columns)}\")\n",
    "print(f\"  After engineering: {len(df_features.columns)}\")\n",
    "print(f\"  New features: {len(df_features.columns) - len(df.columns)}\")\n",
    "\n",
    "# Show new features\n",
    "new_cols = set(df_features.columns) - set(df.columns)\n",
    "print(\"\\n  Created features:\")\n",
    "for col in sorted(new_cols):\n",
    "    print(f\"    - {col}\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nSample of transformed data:\")\n",
    "df_features[list(new_cols)].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## 9. Prepare for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for modeling\n",
    "X, y = engine.prepare_for_modeling(df_features, scale_features=True)\n",
    "\n",
    "print(\"Model-Ready Data:\")\n",
    "print(f\"  Features (X): {X.shape}\")\n",
    "print(f\"  Target (y): {y.shape}\")\n",
    "print(f\"  Feature names: {len(engine.feature_names)}\")\n",
    "\n",
    "print(\"\\nFeature List:\")\n",
    "for i, feat in enumerate(engine.feature_names, 1):\n",
    "    print(f\"{i:2d}. {feat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## 10. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save feature-engineered data\n",
    "import os\n",
    "os.makedirs(config.PROCESSED_DATA_PATH, exist_ok=True)\n",
    "\n",
    "output_path = os.path.join(config.PROCESSED_DATA_PATH, 'absenteeism_features.csv')\n",
    "df_features.to_csv(output_path, index=False)\n",
    "print(f\"✅ Saved feature-engineered data to: {output_path}\")\n",
    "\n",
    "# Save model-ready data\n",
    "X_path = os.path.join(config.PROCESSED_DATA_PATH, 'X_features.csv')\n",
    "y_path = os.path.join(config.PROCESSED_DATA_PATH, 'y_target.csv')\n",
    "\n",
    "X.to_csv(X_path, index=False)\n",
    "y.to_csv(y_path, index=False, header=['Absenteeism time in hours'])\n",
    "\n",
    "print(f\"✅ Saved model-ready features to: {X_path}\")\n",
    "print(f\"✅ Saved target variable to: {y_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### EDA Insights\n",
    "1. **Data Quality:** No missing values, clean dataset\n",
    "2. **Target Distribution:** Right-skewed, mean ~7 hours, median ~3 hours\n",
    "3. **Outliers:** Present but expected (extended medical leave)\n",
    "4. **Correlations:** Weak to moderate correlations with individual features\n",
    "5. **Patterns:** Seasonal and day-of-week variations observed\n",
    "\n",
    "### Transformation Results\n",
    "- ✅ Data cleaned and outliers handled\n",
    "- ✅ 7 new features engineered\n",
    "- ✅ Features encoded and scaled for modeling\n",
    "- ✅ Data saved to `data/processed/`\n",
    "\n",
    "### Next Steps\n",
    "Proceed to `03-aa-feature-engineering.ipynb` for detailed feature analysis, then `04-aa-model-experiments.ipynb` for model training with MLflow."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLOps62",
   "language": "python",
   "name": "mlops62"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}