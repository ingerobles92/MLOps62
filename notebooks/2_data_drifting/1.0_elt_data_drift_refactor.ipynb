{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "9v7rgXQ56h2B",
    "outputId": "17d05152-24c5-4d19-927a-079bbd0fc793"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "# Setup and imports\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add project root to path (handles both Docker /work and local environments)\n",
    "if os.path.exists('/work'):\n",
    "    sys.path.insert(0, '/work')  # Docker environment\n",
    "else:\n",
    "    sys.path.insert(0, os.path.abspath('..'))  # Local environment\n",
    "\n",
    "# ====================================================================================\n",
    "# ===============================================                                   ==\n",
    "# Libraries for dataset verification with DVC. ==                                   ==\n",
    "# ===============================================                                   ==\n",
    "from pathlib import Path  # Cross-platform path handling                            ==\n",
    "from typing import Dict, Tuple, Optional  # Optional type hints for better clarity  ==\n",
    "import os  # File system and environment variable handling                          ==\n",
    "import yaml  # Read .dvc (YAML) pointer files                                       ==\n",
    "import hashlib  # Compute MD5 hashes to verify data integrity                       ==\n",
    "import subprocess    # Execute SO commands                                          ==\n",
    "# ====================================================================================\n",
    "\n",
    "#import de librerias para EDA\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "#importa libraies for data drifting\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from alibi_detect.cd import KSDrift\n",
    "\n",
    "# Import our custom modules\n",
    "from src import config\n",
    "from src.data_utils import load_data\n",
    "from src.plots import (\n",
    "    plot_target_distribution,\n",
    "    plot_correlation_matrix,\n",
    "    create_eda_summary_dashboard,\n",
    "    plot_categorical_analysis,\n",
    "    plot_numerical_relationship\n",
    ")\n",
    "\n",
    "# Import Phase 1 feature engine (from features.py file, not features/ directory)\n",
    "import importlib\n",
    "features_module = importlib.import_module('src.features_engine')\n",
    "AbsenteeismFeatureEngine = features_module.AbsenteeismFeatureEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "zywyJDoQFLlF"
   },
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# DVC dataset verification helpers (MD5 check + pull fallback) ==\n",
    "# ===============================================================\n",
    "\n",
    "def ensure_repo_ready(repo_root: str = \"/work\") -> None:\n",
    "    \"\"\"\n",
    "    Verifies that:\n",
    "    - `repo_root` is a valid project folder with Git and DVC.\n",
    "    - Directory `repo_root` exists.\n",
    "    - It contains a `.git` subdirectory (it's a Git repo).\n",
    "    - It contains a `.dvc` subdirectory (it's a DVC repo).\n",
    "\n",
    "    Raises:\n",
    "    - FileNotFoundError if `repo_root` does not exist.\n",
    "    - RuntimeError if `.git` or `.dvc` is missing.\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(repo_root):\n",
    "        raise FileNotFoundError(f\"Repo root does not exist: {repo_root}\")\n",
    "    if not os.path.isdir(os.path.join(repo_root, \".git\")):\n",
    "        raise RuntimeError(f\"Not a Git repo: {repo_root}\")\n",
    "    if not os.path.isdir(os.path.join(repo_root, \".dvc\")):\n",
    "        raise RuntimeError(f\"Not a DVC repo: {repo_root} (.dvc not found)\")\n",
    "\n",
    "\n",
    "def _md5_file(path: str, chunk_size: int = 1024 * 1024) -> str:\n",
    "    \"\"\"\n",
    "    Computes the MD5 hash of a file by streaming it from disk to verify integrity\n",
    "    against the value stored by DVC in the `.dvc` pointer (default md5-based cache).\n",
    "\n",
    "    Parameters:\n",
    "    - path: absolute file path.\n",
    "    - chunk_size: read block size in bytes (default 1 MB).\n",
    "\n",
    "    Returns:\n",
    "    - Hex MD5 string of the file content.\n",
    "    \"\"\"\n",
    "    h = hashlib.md5()\n",
    "    with open(path, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(chunk_size), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "\n",
    "def _read_expected_md5_from_dvc(pointer_path: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Reads the expected MD5 from a single-file `.dvc` pointer.\n",
    "\n",
    "    `.dvc` format:\n",
    "      - md5: <hash>\n",
    "      - hash: md5\n",
    "      - path: <file_name>\n",
    "\n",
    "    Parameters:\n",
    "    - pointer_path: absolute path to the `.dvc` file.\n",
    "\n",
    "    Returns:\n",
    "    - The MD5 string if present, or None if the pointer does not exist / lacks md5.\n",
    "\n",
    "    Use:\n",
    "    - Compare the expected MD5 from `.dvc` with the actual local file MD5.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(pointer_path):\n",
    "        return None\n",
    "    with open(pointer_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = yaml.safe_load(f) or {}\n",
    "    outs = data.get(\"outs\") or []\n",
    "    if not outs:\n",
    "        return None\n",
    "    out = outs[0]\n",
    "    return out.get(\"md5\") or out.get(\"checksum\") or None\n",
    "\n",
    "\n",
    "def _dvc_pull_target(path_repo_rel: str, repo_root: str = \"/work\") -> None:\n",
    "    \"\"\"\n",
    "    Runs `dvc pull <path>` or `<path>.dvc` to materialize the correct version from the remote (S3)\n",
    "    into the local workspace/cache. Raises if it fails (credentials, permissions, etc.).\n",
    "\n",
    "    Parameters:\n",
    "    - path_repo_rel: repo-relative path to fetch (e.g., \"data/raw/file.csv\").\n",
    "    - repo_root: repo root (e.g., \"/work\").\n",
    "    \"\"\"\n",
    "# Build possible targets\n",
    "    dvc_pointer = os.path.join(repo_root, path_repo_rel + \".dvc\")\n",
    "    if os.path.exists(dvc_pointer):\n",
    "        target = path_repo_rel + \".dvc\"\n",
    "    else:\n",
    "        target = path_repo_rel\n",
    "\n",
    "    # Try pulling\n",
    "    result = subprocess.run(\n",
    "        [\"dvc\", \"pull\", \"--quiet\", target],\n",
    "        cwd=repo_root,\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "    )\n",
    "\n",
    "    # Raise if failed\n",
    "    if result.returncode != 0:\n",
    "        raise RuntimeError(\n",
    "            f\"Failed to run 'dvc pull {target}':\\n\"\n",
    "            f\"STDOUT:\\n{result.stdout}\\nSTDERR:\\n{result.stderr}\"\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "def dvc_read_csv_verified(\n",
    "    path_repo_rel: str,\n",
    "    repo_root: str = \"/work\",\n",
    "    prefer_dvc: bool = False,\n",
    "    verify_local_md5: bool = True,\n",
    "    pandas_read_csv_kwargs: Optional[Dict] = None,\n",
    ") -> Tuple[pd.DataFrame, str]:\n",
    "    \"\"\"\n",
    "    Read a DVC-versioned CSV ensuring integrity when reading locally.\n",
    "\n",
    "    Strategy:\n",
    "    - If `prefer_dvc=True`: force fetching the official version with `dvc pull`\n",
    "      and then read locally. Returns (\"pulled\").\n",
    "    - If `prefer_dvc=False`:\n",
    "        1) If the local file exists and `verify_local_md5=True`, compare local MD5\n",
    "           with the expected MD5 from the `.dvc` pointer.\n",
    "           * If equal -> read local (fast). Returns (\"local\").\n",
    "           * If NOT equal -> run `dvc pull` and read the official version. Returns (\"pulled\").\n",
    "        2) If the file does NOT exist -> run `dvc pull` and read the official version. Returns (\"pulled\").\n",
    "\n",
    "    Parameters:\n",
    "    - path_repo_rel: repo-relative CSV path (e.g., \"data/raw/file.csv\").\n",
    "    - repo_root: repo root (e.g., \"/work\").\n",
    "    - prefer_dvc: if True, ignore local state and fetch official version with `dvc pull`.\n",
    "    - verify_local_md5: if True, validate local MD5 before trusting local read.\n",
    "    - pandas_read_csv_kwargs: kwargs for `pandas.read_csv()` (sep, encoding, etc.).\n",
    "\n",
    "    Returns:\n",
    "    - (df, source) where source ‚àà {\"local\", \"pulled\"} describing the read source.\n",
    "\n",
    "    Exceptions:\n",
    "    - Raises if the file cannot be materialized from the remote (credentials,\n",
    "      permissions, or missing blob).\n",
    "    \"\"\"\n",
    "    ensure_repo_ready(repo_root)\n",
    "    if pandas_read_csv_kwargs is None:\n",
    "        pandas_read_csv_kwargs = {}\n",
    "\n",
    "    local_path = os.path.join(repo_root, path_repo_rel)\n",
    "    dvc_pointer = local_path + \".dvc\"  # e.g., data/raw/file.csv.dvc\n",
    "    expected_md5 = _read_expected_md5_from_dvc(dvc_pointer)\n",
    "\n",
    "    # Option: force ‚Äúofficial‚Äù read by fetching from S3\n",
    "    if prefer_dvc:\n",
    "        _dvc_pull_target(path_repo_rel, repo_root)\n",
    "        # Note: when forcing, we don‚Äôt compare MD5; we assume `dvc pull` fetched the official version.\n",
    "        return pd.read_csv(local_path, **pandas_read_csv_kwargs), \"pulled\"\n",
    "\n",
    "    # If a local file exists, decide based on MD5\n",
    "    if os.path.exists(local_path):\n",
    "        if verify_local_md5 and expected_md5:\n",
    "            try:\n",
    "                md5_local = _md5_file(local_path)\n",
    "                if md5_local == expected_md5:\n",
    "                    # Note: ‚ÄúMD5 OK: local matches .dvc‚Äù\n",
    "                    # Use the local version (faster) because it‚Äôs identical to the ‚Äúofficial‚Äù one.\n",
    "                    return pd.read_csv(local_path, **pandas_read_csv_kwargs), \"local\"\n",
    "                else:\n",
    "                    # MD5 differs: local != .dvc ‚Üí run dvc pull\n",
    "                    _dvc_pull_target(path_repo_rel, repo_root)\n",
    "                    return pd.read_csv(local_path, **pandas_read_csv_kwargs), \"pulled\"\n",
    "            except Exception:\n",
    "                # Any issue during the check ‚Üí ensure consistency with a pull\n",
    "                _dvc_pull_target(path_repo_rel, repo_root)\n",
    "                return pd.read_csv(local_path, **pandas_read_csv_kwargs), \"pulled\"\n",
    "        else:\n",
    "            # Local read without MD5 verification\n",
    "            return pd.read_csv(local_path, **pandas_read_csv_kwargs), \"local\"\n",
    "\n",
    "    # If no local file, fetch the official version\n",
    "    _dvc_pull_target(path_repo_rel, repo_root)\n",
    "    return pd.read_csv(local_path, **pandas_read_csv_kwargs), \"pulled\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "wtT0bvx7FLlG",
    "outputId": "33b40331-3d59-4889-f94e-4b703fb436cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo root: /work | exists: True\n",
      "Expected CSV: data/raw/work_absenteeism_modified.csv\n",
      "Expected MD5 (.dvc): 96c318341d1846f567be7127f52d03e1\n",
      "Local MD5: 96c318341d1846f567be7127f52d03e1\n",
      "MD5 matches .dvc? YES ‚úÖ\n",
      "Read from: local | rows=754 | cols=22\n"
     ]
    }
   ],
   "source": [
    "# =======================================\n",
    "# Configurable dataset read parameters ==\n",
    "# =======================================\n",
    "\n",
    "# Docker mounts the project at /work. If your compose changes, adjust REPO_ROOT accordingly.\n",
    "REPO_ROOT = \"/work\"  # Where the repo is mounted.\n",
    "PATH = \"data/raw/work_absenteeism_modified.csv\"  # Repo-relative path of the DVC-versioned dataset.\n",
    "\n",
    "# Arguments forwarded to pandas.read_csv. Optional: delimiter, encoding, etc.\n",
    "READ_KW: Dict = {}  # e.g.: {\"sep\": \",\", \"encoding\": \"utf-8\"}\n",
    "\n",
    "# Read mode:\n",
    "# - PREFER_DVC=True  -> Force fetching the official version with `dvc pull` and read it.\n",
    "# - PREFER_DVC=False -> Prefer local only if (and only if) MD5 matches the one in the .dvc.\n",
    "PREFER_DVC = False\n",
    "VERIFY_LOCAL_MD5 = True\n",
    "\n",
    "# =====================================================\n",
    "# Environment inspection + demo read with MD5 legend ==\n",
    "# =====================================================\n",
    "print(\"Repo root:\", REPO_ROOT, \"| exists:\", Path(REPO_ROOT).exists())\n",
    "print(\"Expected CSV:\", PATH)\n",
    "\n",
    "# Show expected MD5 (if the pointer exists)\n",
    "pointer_path = os.path.join(REPO_ROOT, PATH) + \".dvc\"\n",
    "expected = _read_expected_md5_from_dvc(pointer_path)\n",
    "print(\"Expected MD5 (.dvc):\", expected or \"<no md5 in pointer>\")\n",
    "\n",
    "# If a local file exists, compute local MD5 and compare\n",
    "local_abs = os.path.join(REPO_ROOT, PATH)\n",
    "if os.path.exists(local_abs) and expected:\n",
    "    try:\n",
    "        local_md5 = _md5_file(local_abs)\n",
    "        print(\"Local MD5:\", local_md5)\n",
    "        print(\"MD5 matches .dvc?\", \"YES ‚úÖ\" if local_md5 == expected else \"NO ‚ùå\")\n",
    "    except Exception as e:\n",
    "        print(\"Could not compute local MD5:\", type(e).__name__, str(e)[:120])\n",
    "\n",
    "# --- Robust read with integrity verification ---\n",
    "# dvc_read_csv_verified does:\n",
    "#   1) If PREFER_DVC=True -> run `dvc pull` and read the official version (‚Äúpulled‚Äù).\n",
    "#   2) If PREFER_DVC=False:\n",
    "#        - If the local file exists and VERIFY_LOCAL_MD5=True:\n",
    "#            compare local MD5 against the MD5 from the .dvc pointer.\n",
    "#            * If equal -> read local (fast) and consistent.\n",
    "#            * If different -> `dvc pull` and read the official version.\n",
    "#        - If the file does not exist locally -> `dvc pull` and read the official version.\n",
    "df, source = dvc_read_csv_verified(\n",
    "    PATH,\n",
    "    repo_root=REPO_ROOT,\n",
    "    prefer_dvc=PREFER_DVC,\n",
    "    verify_local_md5=VERIFY_LOCAL_MD5,\n",
    "    pandas_read_csv_kwargs=READ_KW,\n",
    ")\n",
    "\n",
    "print(f\"Read from: {source} | rows={len(df)} | cols={len(df.columns)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-AAc06_LwIn"
   },
   "source": [
    "**1. Visualizaci√≥n del dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "id": "SgfiE6fM63Sd",
    "outputId": "98d698b7-4477-4d0e-9662-c5ba519ec33a"
   },
   "outputs": [],
   "source": [
    "absenteeism_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "pIT0F1UlyKzw",
    "outputId": "fc9cdc1d-582a-48ed-8147-01509d9b77f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "AN√ÅLISIS DE POTENCIALES VARIABLES TARGET\n",
      "======================================================================\n",
      "\n",
      "üìä Columna: ID\n",
      "   Tipo: object\n",
      "   Nulos: 8 (1.1%)\n",
      "   √önicos: 59\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üìä Columna: Reason for absence\n",
      "   Tipo: object\n",
      "   Nulos: 6 (0.8%)\n",
      "   √önicos: 51\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üìä Columna: Month of absence\n",
      "   Tipo: object\n",
      "   Nulos: 11 (1.5%)\n",
      "   √önicos: 31\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üìä Columna: Day of the week\n",
      "   Tipo: object\n",
      "   Nulos: 8 (1.1%)\n",
      "   √önicos: 21\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üìä Columna: Seasons\n",
      "   Tipo: object\n",
      "   Nulos: 4 (0.5%)\n",
      "   √önicos: 17\n",
      "   Valores: {'4.0': 191, '2.0': 183, '3.0': 173, '1.0': 160, ' 2.0 ': 10, ' 4.0 ': 10, ' 3.0 ': 7, ' 1.0 ': 7, '986.0': 1, ' NAN ': 1, '643.0': 1, '963.0': 1, '866.0': 1, '949.0': 1, '45.0': 1, '79.0': 1, ' 246.0 ': 1}\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üìä Columna: Transportation expense\n",
      "   Tipo: object\n",
      "   Nulos: 8 (1.1%)\n",
      "   √önicos: 49\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üìä Columna: Distance from Residence to Work\n",
      "   Tipo: object\n",
      "   Nulos: 11 (1.5%)\n",
      "   √önicos: 50\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üìä Columna: Service time\n",
      "   Tipo: object\n",
      "   Nulos: 7 (0.9%)\n",
      "   √önicos: 37\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üìä Columna: Age\n",
      "   Tipo: object\n",
      "   Nulos: 5 (0.7%)\n",
      "   √önicos: 40\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üìä Columna: Work load Average/day\n",
      "   Tipo: object\n",
      "   Nulos: 13 (1.7%)\n",
      "   √önicos: 71\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üìä Columna: Hit target\n",
      "   Tipo: object\n",
      "   Nulos: 8 (1.1%)\n",
      "   √önicos: 36\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üìä Columna: Disciplinary failure\n",
      "   Tipo: object\n",
      "   Nulos: 7 (0.9%)\n",
      "   √önicos: 10\n",
      "   Valores: {'0.0': 663, '1.0': 40, ' 0.0 ': 36, 'error': 2, 'invalid': 1, '622.0': 1, '208.0': 1, ' 1.0 ': 1, '228.0': 1, '541.0': 1}\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üìä Columna: Education\n",
      "   Tipo: float64\n",
      "   Nulos: 9 (1.2%)\n",
      "   √önicos: 11\n",
      "   Rango: [1.00, 840.00]\n",
      "   Media: 4.37\n",
      "   Valores: {1.0: 610, 3.0: 78, 2.0: 46, 4.0: 4, 46.0: 1, 486.0: 1, 97.0: 1, 840.0: 1, 34.0: 1, 49.0: 1, 755.0: 1}\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üìä Columna: Son\n",
      "   Tipo: object\n",
      "   Nulos: 12 (1.6%)\n",
      "   √önicos: 19\n",
      "   Valores: {'0.0': 280, '1.0': 209, '2.0': 138, '4.0': 35, ' 1.0 ': 20, ' 0.0 ': 17, '3.0': 13, ' 2.0 ': 12, ' 4.0 ': 7, 'error': 2, ' 23.0 ': 1, '22.0': 1, ' 3.0 ': 1, '828.0': 1, '151.0': 1, '359.0': 1, ' 17.0 ': 1, '130.0': 1, '?': 1}\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üìä Columna: Social drinker\n",
      "   Tipo: object\n",
      "   Nulos: 5 (0.7%)\n",
      "   √önicos: 12\n",
      "   Valores: {'1.0': 408, '0.0': 310, ' 1.0 ': 13, ' 0.0 ': 10, '996.0': 1, '265.0': 1, ' NAN ': 1, '66.0': 1, '78.0': 1, '?': 1, 'invalid': 1, '745.0': 1}\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üìä Columna: Social smoker\n",
      "   Tipo: object\n",
      "   Nulos: 10 (1.3%)\n",
      "   √önicos: 10\n",
      "   Valores: {'0.0': 651, '1.0': 51, ' 0.0 ': 33, ' 1.0 ': 2, 'invalid': 2, '699.0': 1, 'error': 1, '?': 1, '473.0': 1, '804.0': 1}\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üìä Columna: Pet\n",
      "   Tipo: object\n",
      "   Nulos: 12 (1.6%)\n",
      "   √önicos: 21\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üìä Columna: Weight\n",
      "   Tipo: float64\n",
      "   Nulos: 6 (0.8%)\n",
      "   √önicos: 39\n",
      "   Rango: [56.00, 5890.00]\n",
      "   Media: 117.77\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üìä Columna: Height\n",
      "   Tipo: object\n",
      "   Nulos: 5 (0.7%)\n",
      "   √önicos: 31\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üìä Columna: Body mass index\n",
      "   Tipo: object\n",
      "   Nulos: 4 (0.5%)\n",
      "   √önicos: 40\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üìä Columna: Absenteeism time in hours\n",
      "   Tipo: object\n",
      "   Nulos: 10 (1.3%)\n",
      "   √önicos: 37\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üìä Columna: mixed_type_col\n",
      "   Tipo: object\n",
      "   Nulos: 106 (14.1%)\n",
      "   √önicos: 403\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def analyze_potential_targets(absenteeism_df):\n",
    "    \"\"\"Analiza columnas candidatas a ser target.\"\"\"\n",
    "\n",
    "    print(\"=\"*70)\n",
    "    print(\"AN√ÅLISIS DE POTENCIALES VARIABLES TARGET\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    for col in df.columns:\n",
    "        print(f\"\\nüìä Columna: {col}\")\n",
    "        print(f\"   Tipo: {absenteeism_df[col].dtype}\")\n",
    "        print(f\"   Nulos: {absenteeism_df[col].isnull().sum()} ({df[col].isnull().mean()*100:.1f}%)\")\n",
    "        print(f\"   √önicos: {absenteeism_df[col].nunique()}\")\n",
    "\n",
    "        # Si es num√©rica\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            print(f\"   Rango: [{absenteeism_df[col].min():.2f}, {df[col].max():.2f}]\")\n",
    "            print(f\"   Media: {absenteeism_df[col].mean():.2f}\")\n",
    "\n",
    "        # Si es categ√≥rica\n",
    "        if df[col].nunique() < 20:\n",
    "            print(f\"   Valores: {absenteeism_df[col].value_counts().to_dict()}\")\n",
    "\n",
    "        print(\"-\"*70)\n",
    "\n",
    "# Uso:\n",
    "analyze_potential_targets(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "07jP9xFByKzw",
    "outputId": "d652ece7-20b9-4018-ca97-a69a20e9b172"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Validando: Absenteeism time in hours\n",
      "‚úÖ Categ√≥rica: 37 clases\n",
      "\n",
      "Distribuci√≥n:\n",
      "count     744\n",
      "unique     37\n",
      "top       8.0\n",
      "freq      195\n",
      "Name: Absenteeism time in hours, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def validate_target(absenteeism_df, target_col):\n",
    "    \"\"\"Valida si una columna es adecuada como target.\"\"\"\n",
    "\n",
    "    print(f\"\\nüîç Validando: {target_col}\")\n",
    "\n",
    "    # 1. Verificar existencia\n",
    "    if target_col not in absenteeism_df.columns:\n",
    "        print(\"‚ùå La columna no existe\")\n",
    "        return False\n",
    "\n",
    "    # 2. Verificar nulos\n",
    "    null_pct = absenteeism_df[target_col].isnull().mean() * 100\n",
    "    if null_pct > 30:\n",
    "        print(f\"‚ö†Ô∏è Muchos nulos: {null_pct:.1f}%\")\n",
    "\n",
    "    # 3. Verificar variabilidad\n",
    "    n_unique = absenteeism_df[target_col].nunique()\n",
    "    if n_unique == 1:\n",
    "        print(\"‚ùå Sin variabilidad (todos iguales)\")\n",
    "        return False\n",
    "\n",
    "    # 4. Verificar tipo\n",
    "    if pd.api.types.is_numeric_dtype(absenteeism_df[target_col]):\n",
    "        print(f\"‚úÖ Num√©rica: {n_unique} valores √∫nicos\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Categ√≥rica: {n_unique} clases\")\n",
    "\n",
    "    # 5. Distribuci√≥n\n",
    "    print(f\"\\nDistribuci√≥n:\")\n",
    "    if n_unique < 20:\n",
    "        print(absenteeism_df[target_col].value_counts())\n",
    "    else:\n",
    "        print(absenteeism_df[target_col].describe())\n",
    "\n",
    "    return True\n",
    "\n",
    "# Uso:\n",
    "validate_target(absenteeism_df, 'Absenteeism time in hours')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "lcsJGqd_yKzx"
   },
   "outputs": [],
   "source": [
    "def show_dataset_info(absenteeism_df):\n",
    "    \"\"\"Muestra informaci√≥n b√°sica del dataset.\"\"\"\n",
    "    print(\"Primeras filas del dataset:\")\n",
    "    display(absenteeism_df.head())\n",
    "\n",
    "    print(\"\\nEstad√≠sticas descriptivas:\")\n",
    "    display(absenteeism_df.describe().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "s1sdnbO4yKzx"
   },
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# FUNCIONES DE ENTRENAMIENTO Y EVALUACI√ìN\n",
    "# =====================================================================\n",
    "\n",
    "def train_model(X_train, y_train, n_estimators=10, max_depth=10, random_state=42):\n",
    "    \"\"\"Entrena un clasificador Random Forest.\"\"\"\n",
    "    clf = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "    return clf\n",
    "\n",
    "\n",
    "def evaluate_model(clf, X_test, y_test, data_type=\"original\"):\n",
    "    \"\"\"Eval√∫a el modelo y retorna las m√©tricas.\"\"\"\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred, average=\"weighted\"),\n",
    "        'f1': f1_score(y_test, y_pred, average=\"weighted\")\n",
    "    }\n",
    "\n",
    "    print(f'\\nM√©tricas con datos {data_type}:')\n",
    "    print(f'  Accuracy: {metrics[\"accuracy\"]:.2f}')\n",
    "    print(f'  Recall: {metrics[\"recall\"]:.2f}')\n",
    "    print(f'  F1-Score: {metrics[\"f1\"]:.2f}')\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "JV7KVX3KyKzx"
   },
   "outputs": [],
   "source": [
    "\n",
    "# =====================================================================\n",
    "# FUNCIONES DE GENERACI√ìN DE DATOS SINT√âTICOS\n",
    "# =====================================================================\n",
    "\n",
    "def create_gmm_model(data, n_components=5, random_state=0):\n",
    "    \"\"\"Crea y ajusta un modelo GMM a los datos.\"\"\"\n",
    "    gmm = GaussianMixture(n_components=n_components, random_state=random_state)\n",
    "    gmm.fit(data.reshape(-1, 1))\n",
    "    return gmm\n",
    "\n",
    "\n",
    "def modify_gmm_means(gmm, mean_shift):\n",
    "    \"\"\"Modifica las medias del GMM aplicando un desplazamiento.\"\"\"\n",
    "    print(f\"\\nMedias originales del GMM:\\n{gmm.means_}\")\n",
    "\n",
    "    modified_means = gmm.means_.copy()\n",
    "    for i in range(len(modified_means)):\n",
    "        modified_means[i] *= (1 + mean_shift)\n",
    "\n",
    "    gmm.means_ = modified_means\n",
    "    print(f\"\\nMedias modificadas del GMM:\\n{gmm.means_}\")\n",
    "\n",
    "    return gmm\n",
    "\n",
    "\n",
    "def generate_synthetic_data(wine_df, target_column, mean_shift):\n",
    "    \"\"\"Genera datos sint√©ticos modificando una columna con GMM.\"\"\"\n",
    "    # Crear y entrenar GMM\n",
    "    gmm = create_gmm_model(wine_df[target_column].to_numpy())\n",
    "\n",
    "    # Modificar medias\n",
    "    gmm = modify_gmm_means(gmm, mean_shift)\n",
    "\n",
    "    # Generar datos sint√©ticos\n",
    "    synthetic_df = wine_df.copy()\n",
    "    n_samples = int(wine_df[target_column].count())\n",
    "    synthetic_df[target_column] = gmm.sample(n_samples)[0].reshape(-1)\n",
    "\n",
    "    return synthetic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "jediTnPTyKzx"
   },
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# FUNCIONES DE DETECCI√ìN DE DRIFT\n",
    "# =====================================================================\n",
    "\n",
    "def detect_drift(X_reference, X_test, p_val=0.05):\n",
    "    \"\"\"Detecta data drift usando Kolmogorov-Smirnov test.\"\"\"\n",
    "    cd = KSDrift(X_reference, p_val=p_val)\n",
    "    drift_pred = cd.predict(X_test)\n",
    "\n",
    "    is_drift = drift_pred['data']['is_drift']\n",
    "    print(f'\\n¬øDrift detectado?: {is_drift}')\n",
    "\n",
    "    return is_drift, drift_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "TysyKFEPyKzx"
   },
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# FUNCIONES DE VISUALIZACI√ìN\n",
    "# =====================================================================\n",
    "\n",
    "def plot_distribution_comparison(original_data, synthetic_data, column_name):\n",
    "    \"\"\"Compara distribuciones de datos originales vs sint√©ticos.\"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    # Distribuci√≥n original\n",
    "    sns.kdeplot(\n",
    "        original_data[column_name],\n",
    "        label=f'Original Data ({column_name})',\n",
    "        fill=True,\n",
    "        color='blue'\n",
    "    )\n",
    "\n",
    "    # Distribuci√≥n sint√©tica\n",
    "    sns.kdeplot(\n",
    "        synthetic_data[column_name],\n",
    "        label=f'Synthetic Data ({column_name})',\n",
    "        fill=True,\n",
    "        color='red'\n",
    "    )\n",
    "\n",
    "    plt.title(\n",
    "        f'Comparison: Original vs Synthetic Data ({column_name})',\n",
    "        fontsize=16\n",
    "    )\n",
    "    plt.xlabel(f'{column_name} content', fontsize=14)\n",
    "    plt.ylabel('Density', fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "VVH5Js_GyKzx"
   },
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# FUNCI√ìN ORQUESTADORA PRINCIPAL\n",
    "# =====================================================================\n",
    "\n",
    "def run_drift_detection_pipeline(\n",
    "    target_column='Absenteeism time in hours',\n",
    "    mean_shift=0.3,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    p_val=0.05\n",
    "):\n",
    "    \"\"\"\n",
    "    Pipeline completo de detecci√≥n de drift.\n",
    "\n",
    "    Args:\n",
    "        target_column: Columna a modificar para generar datos sint√©ticos\n",
    "        mean_shift: Porcentaje de desplazamiento en las medias del GMM\n",
    "        test_size: Proporci√≥n de datos para test\n",
    "        random_state: Semilla aleatoria\n",
    "        p_val: Valor p para el test de drift\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"INICIO DEL PIPELINE DE DETECCI√ìN DE DRIFT\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "\n",
    "    # 1. Obtener variable target\n",
    "    print(\"\\n[1/7] obtener variables target...\")\n",
    "    y = absenteeism_df[target_column]\n",
    "    X = absenteeism_df.drop(columns=[target_column])\n",
    "\n",
    "    # Limpiar datos - convertir a num√©rico y rellenar NaNs\n",
    "    print(\"\\n[2/7] limpieza general...\")\n",
    "    for col in X.columns:\n",
    "        X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "    X = X.fillna(X.median())\n",
    "    y = pd.to_numeric(y, errors='coerce')\n",
    "    y = y.fillna(y.median())\n",
    "\n",
    "    # 2. Dividir datos\n",
    "    print(\"\\n[3/7] Divisi√≥n de datos...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    # 3. Entrenar modelo\n",
    "    print(\"\\n[4/7] Entrenando modelo...\")\n",
    "    clf = train_model(X_train, y_train, random_state=random_state)\n",
    "\n",
    "    # 4. Evaluar con datos originales\n",
    "    print(\"\\n[5/7] Evaluando con datos originales...\")\n",
    "    original_metrics = evaluate_model(clf, X_test, y_test, \"original\")\n",
    "\n",
    "    # 5. Generar datos sint√©ticos\n",
    "    print(f\"\\n[6/7] Generando datos sint√©ticos (shift={mean_shift})...\")\n",
    "    synthetic_df = generate_synthetic_data(absenteeism_df, target_column, mean_shift)\n",
    "    X_synthetic = synthetic_df[wine.feature_names].values\n",
    "    y_synthetic = synthetic_df['target'].values\n",
    "\n",
    "    # 6. Evaluar con datos sint√©ticos\n",
    "    print(\"\\n[7/7] Evaluando con datos sint√©ticos...\")\n",
    "    synthetic_metrics = evaluate_model(clf, X_synthetic, y_synthetic, \"synthetic\")\n",
    "\n",
    "    # 7. Detectar drift\n",
    "    print(\"\\n[8/7] Detectando drift...\")\n",
    "    is_drift, drift_pred = detect_drift(X_train, X_synthetic, p_val=p_val)\n",
    "\n",
    "    # 8. Visualizar comparaci√≥n\n",
    "    print(\"\\n[VISUALIZACI√ìN] Generando gr√°fico de comparaci√≥n...\")\n",
    "    plot_distribution_comparison(absenteeism_df, synthetic_df, target_column)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PIPELINE COMPLETADO\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Retornar resultados\n",
    "    return {\n",
    "        'df': absenteeism_df,\n",
    "        'synthetic_df': synthetic_df,\n",
    "        'model': clf,\n",
    "        'original_metrics': original_metrics,\n",
    "        'synthetic_metrics': synthetic_metrics,\n",
    "        'drift_detected': is_drift,\n",
    "        'drift_prediction': drift_pred\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "uiZ8W5T3yKzx",
    "outputId": "afa59e44-5467-4203-a45a-d5e7e091bd6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "INICIO DEL PIPELINE DE DETECCI√ìN DE DRIFT\n",
      "======================================================================\n",
      "\n",
      "[1/7] obtener variables target...\n",
      "\n",
      "[2/7] limpieza general...\n",
      "\n",
      "[3/7] Divisi√≥n de datos...\n",
      "\n",
      "[4/7] Entrenando modelo...\n",
      "\n",
      "[5/7] Evaluando con datos originales...\n",
      "\n",
      "M√©tricas con datos original:\n",
      "  Accuracy: 0.48\n",
      "  Recall: 0.48\n",
      "  F1-Score: 0.44\n",
      "\n",
      "[6/7] Generando datos sint√©ticos (shift=0.3)...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'error'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# =====================================================================\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# EJECUCI√ìN\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# =====================================================================\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Ejecutar pipeline con par√°metros por defecto\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_drift_detection_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#target_column='Absenteeism time in hours',\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmean_shift\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mp_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 58\u001b[0m, in \u001b[0;36mrun_drift_detection_pipeline\u001b[0;34m(target_column, mean_shift, test_size, random_state, p_val)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# 5. Generar datos sint√©ticos\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[6/7] Generando datos sint√©ticos (shift=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_shift\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 58\u001b[0m synthetic_df \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_synthetic_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mabsenteeism_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_column\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean_shift\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m X_synthetic \u001b[38;5;241m=\u001b[39m synthetic_df[wine\u001b[38;5;241m.\u001b[39mfeature_names]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     60\u001b[0m y_synthetic \u001b[38;5;241m=\u001b[39m synthetic_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n",
      "Cell \u001b[0;32mIn[9], line 29\u001b[0m, in \u001b[0;36mgenerate_synthetic_data\u001b[0;34m(wine_df, target_column, mean_shift)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Genera datos sint√©ticos modificando una columna con GMM.\"\"\"\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Crear y entrenar GMM\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m gmm \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_gmm_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwine_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtarget_column\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Modificar medias\u001b[39;00m\n\u001b[1;32m     32\u001b[0m gmm \u001b[38;5;241m=\u001b[39m modify_gmm_means(gmm, mean_shift)\n",
      "Cell \u001b[0;32mIn[9], line 8\u001b[0m, in \u001b[0;36mcreate_gmm_model\u001b[0;34m(data, n_components, random_state)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Crea y ajusta un modelo GMM a los datos.\"\"\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m gmm \u001b[38;5;241m=\u001b[39m GaussianMixture(n_components\u001b[38;5;241m=\u001b[39mn_components, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mgmm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m gmm\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/sklearn/mixture/_base.py:181\u001b[0m, in \u001b[0;36mBaseMixture.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Estimate model parameters with the EM algorithm.\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03mThe method fits the model ``n_init`` times and sets the parameters with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m    The fitted mixture.\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m# parameters are validated in fit_predict\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/sklearn/mixture/_base.py:212\u001b[0m, in \u001b[0;36mBaseMixture.fit_predict\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    186\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Estimate model parameters using X and predict the labels for X.\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \n\u001b[1;32m    188\u001b[0m \u001b[38;5;124;03m    The method fits the model n_init times and sets the parameters with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03m        Component labels.\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components:\n\u001b[1;32m    214\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    215\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected n_samples >= n_components \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    216\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got n_components = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    217\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    218\u001b[0m         )\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/sklearn/base.py:604\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    602\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 604\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[1;32m    606\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/sklearn/utils/validation.py:917\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    915\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    916\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 917\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[1;32m    921\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/sklearn/utils/_array_api.py:380\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[1;32m    378\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 380\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'error'"
     ]
    }
   ],
   "source": [
    "# =====================================================================\n",
    "# EJECUCI√ìN\n",
    "# =====================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Ejecutar pipeline con par√°metros por defecto\n",
    "    results = run_drift_detection_pipeline(\n",
    "        #target_column='Absenteeism time in hours',\n",
    "        mean_shift=0.3,\n",
    "        test_size=0.3,\n",
    "        random_state=42,\n",
    "        p_val=0.05\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jjS2z3mLyKzx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
