{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Model Experiments with MLflow\n",
    "\n",
    "**Author:** Alexis Alduncin (Data Scientist)\n",
    "**Team:** MLOps 62\n",
    "\n",
    "This notebook trains baseline models with MLflow experiment tracking, comparing Linear Regression and Random Forest approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": "# Setup and imports\nimport sys\nimport os\n\n# Add project root to path (handles both Docker /work and local environments)\nif os.path.exists('/work'):\n    sys.path.insert(0, '/work')  # Docker environment\nelse:\n    sys.path.insert(0, os.path.abspath('..'))  # Local environment\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# ML libraries\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\n# MLflow\nimport mlflow\nimport mlflow.sklearn\n\n# Custom modules\nfrom src import config\nfrom src.data_utils import load_data\nfrom src.plots import plot_model_performance, plot_feature_importance\n\n# Import Phase 1 feature engine (from features.py file, not features/ directory)\nimport importlib\nfeatures_module = importlib.import_module('src.features')\nAbsenteeismFeatureEngine = features_module.AbsenteeismFeatureEngine\n\nprint(\"✅ Modules imported successfully\")\nprint(f\"MLflow Experiment: {config.MLFLOW_EXPERIMENT_NAME}\")"
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 1. MLflow Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set MLflow tracking URI\n",
    "mlflow.set_tracking_uri(config.MLFLOW_TRACKING_URI)\n",
    "\n",
    "# Create or set experiment\n",
    "experiment = mlflow.set_experiment(config.MLFLOW_EXPERIMENT_NAME)\n",
    "\n",
    "print(f\"MLflow Tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "print(f\"Experiment Name: {experiment.name}\")\n",
    "print(f\"Experiment ID: {experiment.experiment_id}\")\n",
    "print(f\"Artifact Location: {experiment.artifact_location}\")\n",
    "\n",
    "print(\"\\n✅ MLflow configured successfully\")\n",
    "print(\"\\nTo view experiments, run: mlflow ui --port 5001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": "# Load data using team's robust DVC approach\ndf_raw = load_data(config.RAW_DATA_PATH)\nprint(f\"Raw data loaded: {df_raw.shape}\")\n\n# Full feature engineering pipeline\nengine = AbsenteeismFeatureEngine()\ndf_clean = engine.clean_data(df_raw)\ndf_features = engine.engineer_features(df_clean)\n\nprint(f\"Cleaned data: {df_clean.shape}\")\nprint(f\"Feature-engineered data: {df_features.shape}\")\n\n# Prepare for modeling\nX, y = engine.prepare_for_modeling(df_features, scale_features=True)\n\nprint(f\"\\nModel-ready data:\")\nprint(f\"  Features (X): {X.shape}\")\nprint(f\"  Target (y): {y.shape}\")\nprint(f\"  Feature count: {len(engine.feature_names)}\")"
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## 3. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=config.TEST_SIZE, \n",
    "    random_state=config.RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(\"Data Split:\")\n",
    "print(f\"  Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"  Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"\\nTarget Statistics:\")\n",
    "print(f\"  Train mean: {y_train.mean():.2f}h, std: {y_train.std():.2f}h\")\n",
    "print(f\"  Test mean: {y_test.mean():.2f}h, std: {y_test.std():.2f}h\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 4. Experiment 1: Linear Regression\n",
    "\n",
    "**Model:** Simple baseline using Linear Regression\n",
    "**Purpose:** Establish baseline performance and identify linear relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start MLflow run\n",
    "with mlflow.start_run(run_name=\"Linear_Regression_Baseline\") as run:\n",
    "    print(f\"MLflow Run ID: {run.info.run_id}\")\n",
    "    \n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"model_type\", \"LinearRegression\")\n",
    "    mlflow.log_param(\"features\", len(engine.feature_names))\n",
    "    mlflow.log_param(\"train_samples\", len(X_train))\n",
    "    mlflow.log_param(\"test_samples\", len(X_test))\n",
    "    mlflow.log_param(\"random_state\", config.RANDOM_STATE)\n",
    "    \n",
    "    # Train model\n",
    "    lr_model = LinearRegression()\n",
    "    lr_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_train_pred = lr_model.predict(X_train)\n",
    "    y_test_pred = lr_model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics - Training\n",
    "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    \n",
    "    # Calculate metrics - Test\n",
    "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"train_mae\", train_mae)\n",
    "    mlflow.log_metric(\"train_rmse\", train_rmse)\n",
    "    mlflow.log_metric(\"train_r2\", train_r2)\n",
    "    mlflow.log_metric(\"test_mae\", test_mae)\n",
    "    mlflow.log_metric(\"test_rmse\", test_rmse)\n",
    "    mlflow.log_metric(\"test_r2\", test_r2)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(lr_model, X_train, y_train, \n",
    "                                 cv=5, scoring='neg_mean_absolute_error')\n",
    "    cv_mae = -cv_scores.mean()\n",
    "    mlflow.log_metric(\"cv_mae_mean\", cv_mae)\n",
    "    mlflow.log_metric(\"cv_mae_std\", cv_scores.std())\n",
    "    \n",
    "    # Log model\n",
    "    mlflow.sklearn.log_model(lr_model, \"model\")\n",
    "    \n",
    "    # Save feature names\n",
    "    mlflow.log_dict({\"features\": engine.feature_names}, \"feature_names.json\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"LINEAR REGRESSION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nTraining Metrics:\")\n",
    "    print(f\"  MAE:  {train_mae:.3f} hours\")\n",
    "    print(f\"  RMSE: {train_rmse:.3f} hours\")\n",
    "    print(f\"  R2:   {train_r2:.3f}\")\n",
    "    print(\"\\nTest Metrics:\")\n",
    "    print(f\"  MAE:  {test_mae:.3f} hours\")\n",
    "    print(f\"  RMSE: {test_rmse:.3f} hours\")\n",
    "    print(f\"  R2:   {test_r2:.3f}\")\n",
    "    print(\"\\nCross-Validation:\")\n",
    "    print(f\"  CV MAE: {cv_mae:.3f} ± {cv_scores.std():.3f} hours\")\n",
    "    \n",
    "    # Check target metrics\n",
    "    print(\"\\nTarget Achievement:\")\n",
    "    print(f\"  MAE < 4h: {'✅ Yes' if test_mae < 4 else '❌ No'} ({test_mae:.2f}h)\")\n",
    "    print(f\"  RMSE < 8h: {'✅ Yes' if test_rmse < 8 else '❌ No'} ({test_rmse:.2f}h)\")\n",
    "    print(f\"  R2 > 0.3: {'✅ Yes' if test_r2 > 0.3 else '❌ No'} ({test_r2:.3f})\")\n",
    "\n",
    "# Visualize performance\n",
    "fig = plot_model_performance(y_test, y_test_pred, \"Linear Regression\")\n",
    "plt.show()\n",
    "\n",
    "# Store results for comparison\n",
    "lr_results = {\n",
    "    'model': lr_model,\n",
    "    'predictions': y_test_pred,\n",
    "    'test_mae': test_mae,\n",
    "    'test_rmse': test_rmse,\n",
    "    'test_r2': test_r2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## 5. Experiment 2: Random Forest\n",
    "\n",
    "**Model:** Random Forest Regressor\n",
    "**Purpose:** Capture non-linear relationships and feature interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start MLflow run\n",
    "with mlflow.start_run(run_name=\"Random_Forest_Baseline\") as run:\n",
    "    print(f\"MLflow Run ID: {run.info.run_id}\")\n",
    "    \n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"model_type\", \"RandomForestRegressor\")\n",
    "    mlflow.log_param(\"n_estimators\", config.RF_N_ESTIMATORS)\n",
    "    mlflow.log_param(\"max_depth\", config.RF_MAX_DEPTH)\n",
    "    mlflow.log_param(\"min_samples_split\", config.RF_MIN_SAMPLES_SPLIT)\n",
    "    mlflow.log_param(\"min_samples_leaf\", config.RF_MIN_SAMPLES_LEAF)\n",
    "    mlflow.log_param(\"random_state\", config.RANDOM_STATE)\n",
    "    mlflow.log_param(\"features\", len(engine.feature_names))\n",
    "    mlflow.log_param(\"train_samples\", len(X_train))\n",
    "    mlflow.log_param(\"test_samples\", len(X_test))\n",
    "    \n",
    "    # Train model\n",
    "    rf_model = RandomForestRegressor(\n",
    "        n_estimators=config.RF_N_ESTIMATORS,\n",
    "        max_depth=config.RF_MAX_DEPTH,\n",
    "        min_samples_split=config.RF_MIN_SAMPLES_SPLIT,\n",
    "        min_samples_leaf=config.RF_MIN_SAMPLES_LEAF,\n",
    "        random_state=config.RANDOM_STATE,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_train_pred = rf_model.predict(X_train)\n",
    "    y_test_pred = rf_model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics - Training\n",
    "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    \n",
    "    # Calculate metrics - Test\n",
    "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"train_mae\", train_mae)\n",
    "    mlflow.log_metric(\"train_rmse\", train_rmse)\n",
    "    mlflow.log_metric(\"train_r2\", train_r2)\n",
    "    mlflow.log_metric(\"test_mae\", test_mae)\n",
    "    mlflow.log_metric(\"test_rmse\", test_rmse)\n",
    "    mlflow.log_metric(\"test_r2\", test_r2)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(rf_model, X_train, y_train, \n",
    "                                 cv=5, scoring='neg_mean_absolute_error')\n",
    "    cv_mae = -cv_scores.mean()\n",
    "    mlflow.log_metric(\"cv_mae_mean\", cv_mae)\n",
    "    mlflow.log_metric(\"cv_mae_std\", cv_scores.std())\n",
    "    \n",
    "    # Feature importance\n",
    "    importance_df = engine.get_feature_importance(rf_model)\n",
    "    mlflow.log_dict(importance_df.to_dict(), \"feature_importance.json\")\n",
    "    \n",
    "    # Log model\n",
    "    mlflow.sklearn.log_model(rf_model, \"model\")\n",
    "    \n",
    "    # Save feature names\n",
    "    mlflow.log_dict({\"features\": engine.feature_names}, \"feature_names.json\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RANDOM FOREST RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nTraining Metrics:\")\n",
    "    print(f\"  MAE:  {train_mae:.3f} hours\")\n",
    "    print(f\"  RMSE: {train_rmse:.3f} hours\")\n",
    "    print(f\"  R2:   {train_r2:.3f}\")\n",
    "    print(\"\\nTest Metrics:\")\n",
    "    print(f\"  MAE:  {test_mae:.3f} hours\")\n",
    "    print(f\"  RMSE: {test_rmse:.3f} hours\")\n",
    "    print(f\"  R2:   {test_r2:.3f}\")\n",
    "    print(\"\\nCross-Validation:\")\n",
    "    print(f\"  CV MAE: {cv_mae:.3f} ± {cv_scores.std():.3f} hours\")\n",
    "    \n",
    "    # Check target metrics\n",
    "    print(\"\\nTarget Achievement:\")\n",
    "    print(f\"  MAE < 4h: {'✅ Yes' if test_mae < 4 else '❌ No'} ({test_mae:.2f}h)\")\n",
    "    print(f\"  RMSE < 8h: {'✅ Yes' if test_rmse < 8 else '❌ No'} ({test_rmse:.2f}h)\")\n",
    "    print(f\"  R2 > 0.3: {'✅ Yes' if test_r2 > 0.3 else '❌ No'} ({test_r2:.3f})\")\n",
    "\n",
    "# Visualize performance\n",
    "fig = plot_model_performance(y_test, y_test_pred, \"Random Forest\")\n",
    "plt.show()\n",
    "\n",
    "# Visualize feature importance\n",
    "fig = plot_feature_importance(importance_df, top_n=20)\n",
    "plt.show()\n",
    "\n",
    "# Store results for comparison\n",
    "rf_results = {\n",
    "    'model': rf_model,\n",
    "    'predictions': y_test_pred,\n",
    "    'test_mae': test_mae,\n",
    "    'test_rmse': test_rmse,\n",
    "    'test_r2': test_r2,\n",
    "    'importance': importance_df\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## 6. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Linear Regression', 'Random Forest'],\n",
    "    'MAE': [lr_results['test_mae'], rf_results['test_mae']],\n",
    "    'RMSE': [lr_results['test_rmse'], rf_results['test_rmse']],\n",
    "    'R2': [lr_results['test_r2'], rf_results['test_r2']]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Determine best model\n",
    "best_model_idx = comparison_df['MAE'].idxmin()\n",
    "best_model_name = comparison_df.loc[best_model_idx, 'Model']\n",
    "best_mae = comparison_df.loc[best_model_idx, 'MAE']\n",
    "\n",
    "print(f\"\\n✅ Best Model: {best_model_name} (MAE: {best_mae:.3f}h)\")\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "metrics = ['MAE', 'RMSE', 'R2']\n",
    "colors = ['skyblue', 'lightcoral']\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    axes[i].bar(comparison_df['Model'], comparison_df[metric], color=colors, edgecolor='black', alpha=0.7)\n",
    "    axes[i].set_ylabel(metric)\n",
    "    axes[i].set_title(f'{metric} Comparison')\n",
    "    axes[i].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for j, v in enumerate(comparison_df[metric]):\n",
    "        axes[i].text(j, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # Highlight best\n",
    "    if metric in ['MAE', 'RMSE']:\n",
    "        best_idx = comparison_df[metric].idxmin()\n",
    "    else:  # R2\n",
    "        best_idx = comparison_df[metric].idxmax()\n",
    "    axes[i].patches[best_idx].set_edgecolor('green')\n",
    "    axes[i].patches[best_idx].set_linewidth(3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Improvement analysis\n",
    "mae_improvement = ((lr_results['test_mae'] - rf_results['test_mae']) / lr_results['test_mae']) * 100\n",
    "rmse_improvement = ((lr_results['test_rmse'] - rf_results['test_rmse']) / lr_results['test_rmse']) * 100\n",
    "r2_improvement = ((rf_results['test_r2'] - lr_results['test_r2']) / abs(lr_results['test_r2'])) * 100\n",
    "\n",
    "print(\"\\nRandom Forest vs Linear Regression:\")\n",
    "print(f\"  MAE improvement: {mae_improvement:+.1f}%\")\n",
    "print(f\"  RMSE improvement: {rmse_improvement:+.1f}%\")\n",
    "print(f\"  R2 improvement: {r2_improvement:+.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## 7. Best Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model based on MAE\n",
    "if rf_results['test_mae'] < lr_results['test_mae']:\n",
    "    best_model = rf_results['model']\n",
    "    best_predictions = rf_results['predictions']\n",
    "    model_name = \"Random Forest\"\n",
    "    has_importance = True\n",
    "else:\n",
    "    best_model = lr_results['model']\n",
    "    best_predictions = lr_results['predictions']\n",
    "    model_name = \"Linear Regression\"\n",
    "    has_importance = False\n",
    "\n",
    "print(f\"Selected Best Model: {model_name}\")\n",
    "\n",
    "# Detailed error analysis\n",
    "errors = y_test - best_predictions\n",
    "abs_errors = np.abs(errors)\n",
    "\n",
    "print(\"\\nError Analysis:\")\n",
    "print(f\"  Mean Error: {errors.mean():.3f}h (bias)\")\n",
    "print(f\"  Std Error: {errors.std():.3f}h\")\n",
    "print(f\"  Max Overestimate: {errors.min():.3f}h\")\n",
    "print(f\"  Max Underestimate: {errors.max():.3f}h\")\n",
    "\n",
    "# Prediction accuracy ranges\n",
    "print(\"\\nPrediction Accuracy:\")\n",
    "within_2h = (abs_errors <= 2).sum() / len(abs_errors) * 100\n",
    "within_4h = (abs_errors <= 4).sum() / len(abs_errors) * 100\n",
    "within_8h = (abs_errors <= 8).sum() / len(abs_errors) * 100\n",
    "\n",
    "print(f\"  Within 2h: {within_2h:.1f}%\")\n",
    "print(f\"  Within 4h: {within_4h:.1f}%\")\n",
    "print(f\"  Within 8h: {within_8h:.1f}%\")\n",
    "\n",
    "# Visualize error distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Error histogram\n",
    "axes[0].hist(errors, bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(0, color='red', linestyle='--', linewidth=2, label='Perfect Prediction')\n",
    "axes[0].axvline(errors.mean(), color='green', linestyle='--', linewidth=2, label=f'Mean Error: {errors.mean():.2f}h')\n",
    "axes[0].set_xlabel('Prediction Error (hours)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title(f'{model_name} - Error Distribution')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Absolute error by actual value\n",
    "axes[1].scatter(y_test, abs_errors, alpha=0.5, s=50)\n",
    "axes[1].axhline(4, color='orange', linestyle='--', label='Target: 4h MAE')\n",
    "axes[1].set_xlabel('Actual Absenteeism (hours)')\n",
    "axes[1].set_ylabel('Absolute Error (hours)')\n",
    "axes[1].set_title('Error vs Actual Values')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Top features if Random Forest\n",
    "if has_importance:\n",
    "    print(\"\\nTop 10 Most Important Features:\")\n",
    "    print(rf_results['importance'].head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## 8. Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "\n",
    "# Create models directory\n",
    "models_dir = os.path.join('..', 'models')\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Save best model\n",
    "model_path = os.path.join(models_dir, f'best_model_{model_name.lower().replace(\" \", \"_\")}.pkl')\n",
    "joblib.dump(best_model, model_path)\n",
    "\n",
    "print(f\"✅ Best model saved to: {model_path}\")\n",
    "\n",
    "# Save feature names\n",
    "feature_path = os.path.join(models_dir, 'feature_names.txt')\n",
    "with open(feature_path, 'w') as f:\n",
    "    for feat in engine.feature_names:\n",
    "        f.write(f\"{feat}\\n\")\n",
    "\n",
    "print(f\"✅ Feature names saved to: {feature_path}\")\n",
    "\n",
    "# Save model metadata\n",
    "metadata = {\n",
    "    'model_type': model_name,\n",
    "    'test_mae': float(best_predictions.mean()),\n",
    "    'test_rmse': float(np.sqrt(mean_squared_error(y_test, best_predictions))),\n",
    "    'test_r2': float(r2_score(y_test, best_predictions)),\n",
    "    'train_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'features_count': len(engine.feature_names),\n",
    "    'training_samples': len(X_train),\n",
    "    'test_samples': len(X_test)\n",
    "}\n",
    "\n",
    "metadata_path = os.path.join(models_dir, 'model_metadata.json')\n",
    "import json\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"✅ Model metadata saved to: {metadata_path}\")\n",
    "print(\"\\nModel artifacts ready for deployment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Experiments Completed:\n",
    "\n",
    "1. **Linear Regression Baseline**\n",
    "   - Simple interpretable model\n",
    "   - Establishes baseline performance\n",
    "   - Captures linear relationships\n",
    "\n",
    "2. **Random Forest Regressor**\n",
    "   - Captures non-linear patterns\n",
    "   - Feature importance analysis\n",
    "   - Better predictive performance\n",
    "\n",
    "### MLflow Tracking:\n",
    "- ✅ Experiment created: `absenteeism-team62`\n",
    "- ✅ All runs logged with parameters and metrics\n",
    "- ✅ Models saved as artifacts\n",
    "- ✅ Feature importance tracked\n",
    "- ✅ Cross-validation results logged\n",
    "\n",
    "### Best Model:\n",
    "Selected based on lowest test MAE\n",
    "\n",
    "### Model Artifacts:\n",
    "- ✅ Best model saved to `models/`\n",
    "- ✅ Feature names documented\n",
    "- ✅ Metadata saved for reproducibility\n",
    "\n",
    "### Next Steps:\n",
    "1. Review MLflow UI for experiment comparison\n",
    "2. Consider hyperparameter tuning for Phase 2\n",
    "3. Explore ensemble methods\n",
    "4. Prepare model for deployment\n",
    "\n",
    "**To view experiments:** Run `mlflow ui --port 5001` and navigate to http://localhost:5001"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}