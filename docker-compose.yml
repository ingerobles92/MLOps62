services:
  mlops-app:
    build: .
    container_name: mlops-app
    working_dir: /work
    volumes:
      - ./:/work
      - ${HOME}/.ssh:/root/.ssh:ro
      - ${HOME}/.gitconfig:/root/.gitconfig:ro
    env_file:
      - .env
    environment:
      # MLflow client inside notebooks/scripts
      MLFLOW_TRACKING_URI: http://mlflow:9001
    ports:
      - "8888:8888"
    command: bash -lc "\
      jupyter lab --ip=0.0.0.0 --no-browser --port=8888 --NotebookApp.token='' --allow-root & \
      tail -f /dev/null"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  mlflow:
    image: python:3.13-slim
    container_name: mlflow
    working_dir: /work
    volumes:
      - ./.mlflow:/work/.mlflow
    env_file:
      - .env
    environment:
      # ‚Üê Permite llamadas desde otros contenedores y el host
      MLFLOW_SERVER_ALLOWED_HOSTS: "mlflow:*,localhost:*,127.0.0.1,host.docker.internal:*"
      # (Opcional) CORS amplio si usas front-ends:
      # MLFLOW_SERVER_CORS_ALLOWED_ORIGINS: "*"
    ports:
      - "9001:9001"
    command: bash -lc "\
      pip install --no-cache-dir --upgrade pip && \
      pip install --no-cache-dir mlflow boto3 s3fs && \
      mlflow server \
        --host 0.0.0.0 --port 9001 \
        --backend-store-uri sqlite:///./.mlflow/mlflow.db \
        --default-artifact-root s3://mlopsequipo62/mlops/artifacts"

  ml-abs-service:
    build: .
    image: python:3.13-slim
    container_name: ml-abs-service
    working_dir: /work
    volumes:
      - ./:/work
    env_file:
      - .env
    ports:
      - "8080:8080"
    command: bash -lc "\
      pip install --no-cache-dir \"fastapi[standard]\" && \
      pip install --no-cache-dir uvicorn && \
      cd src/app && \
      uvicorn main:app --host 0.0.0.0 --port 8080"